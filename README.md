In this assignment, II tried to solve a classification problem using NLP. Naive Bayes Algorithm is used for classification, n-gram models is used for Bow.

First I imported the data and made some visualizations and summaries to understand data better. Then I splitted my data as train and test data with 0.8 train ratio and 0.2 test ratio. Then I implemented Ngram algorithms. I have 2 ngram methods, one is considering punctuation marks as word, the other is not. Then I impleneted a method to remove stop words. I created an array for stop/unnecessary words and in the method, I removed the stop words from parameter text. Then, I implemented create_bow method to create my dictionaries. In this method, first I removed all stopwords from review. Then I used one of the ngram methods according to the parameters that shows punctuation is used and n number for ngrams. Then I implemented logarithmic probabilities calculation method and naive bayes classification method. In naive bayes, unknown probabilities are used too. Finally, I implemented accuracy precision and recall calculation methods to evaluate my models performance.

After the implementation parts, I started to testing part. I used 6 test cases as "Unigram With Punctuation, Unigram With No Punctuation, Bigram With Punctuation, Bigram With No Punctuation, Trigram With Punctuation, Trigram With No Punctuation". I created bow dict and calculated vocab size, positive/negative logaritmic probabilities and positive/negative unknown probabilities for each cases. Then I evaluated their performances.

Unigram With No Punctuation gets highest accuracy and F1 score. Unigram With Punctuation has the second best accuracy, but the lowest F1 score. Bigram and Trigram results are very close to each other, approximately the same values. Their accuracies are lower than Unigram accuracies, but they have better recall values. According to the results, Unigram is the best ngram type for that project, especially when punctuations are not considered as words. For bigram and trigram, punctuation made very small changes. So I could say that one word focused dictionary is better.

To get higher accuracies; more unnecessary words may be added to stop word array, text may be preprocessed, TFIDF may be used instead of Bow for extracting, some other models especially neural network based models may be used instead of Naive Bayes.

Dataset link: https://ai.stanford.edu/~amaas/data/sentiment/
